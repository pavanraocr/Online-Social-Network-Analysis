About the assignment A4:

The jist of the entire assignment is to choose what people are talking about the california exit that after the Donald Trump became the president-elect. The following is the approch and description of all the scripts what they intend to do and what have they achieved. The assignment has 3 phases to it and each phase is done in each of the files all the logs of executing each is also provided. The first step is to collect the data. This basically collects all the recent tweets from the twitter API on predefined HASHTAGS which is defined as a global variable in the collect.py. Since there is a rate limit enforced on the tweet extraction we do it in batches of 180 tweets per requests. Every time a response is acquired we write it to a binary file by using pickle library. The scripts stops the data collection once either the user limit or the tweet limit is reached these are the variables that are defined as global variables in the file. We can change this values to get different number of tweets. The number of users are bascially a unique list of the users who tweeted the tweets which are continuesly being collected. This information is stored in the files given the global constants in the collect.py.

Now that the data is collected. We have 2 set of information. One is the tweets on a #calexit hashtag, second is the list of users who tweeted the previously collected tweets. We now go on to cluster.py. In this file we first find the social links between any 2 users in the list of users. I do this by fetching friends/ids of each users that I collected and create an edge between 2 users if they have any common friends between them. Now this will result in a network of users which is plotted by networkx library and both pre clustering and post clustering graph is drawn and stored in the pre_network.png and post_network.png files respectively. Once the initial network is created we cluster the graph using girvan newman method. The number of clusters formed is predefined in a global variable and these formed based on the similarity between 2 users.

Once the clusters are formed. We now move on to the next file which is the classification of the tweets that we previously gathrered. We try to classify the tweets using the logistic regression we fit our model based on the training data which has been manually categorized into positive and negative classes. Thus now we establish the fact that there exists only 2 classes in our classifier where 0 denotes negative and 1 denotes positive. Since the tweets were manually classified I was not able to collect a lot of tweets and label them so the classifier might be a little week in classifying new set of tweets. This file requires train_tweets.txt and train_users.txt for successfull execution. Finally once these are categorized we can have get a hold of general notion about the topic provided.

Future improvisations can be done to the classification file where we find out the predicted class lables for each tweets and who tweeted them and we can average out count of the labels of the same community which we found out in the cluster.py. Now this will give us more informed sentiments of the users are they are averaged based on the similar users.

Thank you for reading
